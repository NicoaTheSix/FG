{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffe3f900",
   "metadata": {},
   "source": [
    "## Task Definition\n",
    "- input:\n",
    "- ouptut:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520c266",
   "metadata": {},
   "source": [
    "## RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99552cbc",
   "metadata": {},
   "source": [
    "#### 子功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463befc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "\n",
    "class AuditLogEnv(gym.Env):\n",
    "    def __init__(self, session_logs, max_steps=50):\n",
    "        super().__init__()\n",
    "\n",
    "        self.logs = session_logs\n",
    "        self.max_steps = max_steps\n",
    "\n",
    "        # ===== Action Space =====\n",
    "        self.action_space = spaces.Discrete(20)\n",
    "\n",
    "        # ===== Observation Space =====\n",
    "        # 你先固定 state 向量長度，例如 64\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(64,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.cursor = 0\n",
    "        self.window_size = 5\n",
    "        self.steps = 0\n",
    "\n",
    "        self.labeled_logs = {}\n",
    "        self.seen_entities = set()\n",
    "        self.done = False\n",
    "\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        self.steps += 1\n",
    "        reward = -0.01  # 每一步都有成本\n",
    "\n",
    "        if action == 0:\n",
    "            pass  # SKIP\n",
    "\n",
    "        elif 1 <= action <= 10:\n",
    "            reward += self._handle_label(action)\n",
    "\n",
    "        elif 11 <= action <= 15:\n",
    "            reward += self._handle_search(action)\n",
    "\n",
    "        elif 16 <= action <= 18:\n",
    "            self._handle_expand(action)\n",
    "\n",
    "        elif action == 19:\n",
    "            reward += self._final_reward()\n",
    "            self.done = True\n",
    "\n",
    "        if self.steps >= self.max_steps:\n",
    "            self.done = True\n",
    "\n",
    "        return self._get_state(), reward, self.done, {}\n",
    "\n",
    "    # ===== 以下是環境邏輯 =====\n",
    "\n",
    "    def _handle_label(self, action):\n",
    "        idx = (action - 1) // 2\n",
    "        label = (action - 1) % 2  # 0 benign, 1 malicious\n",
    "\n",
    "        candidates = self._topk_logs()\n",
    "        if idx >= len(candidates):\n",
    "            return -0.1\n",
    "\n",
    "        log_id = candidates[idx][\"id\"]\n",
    "        self.labeled_logs[log_id] = label\n",
    "\n",
    "        # reward shaping（示意）\n",
    "        return 0.2 if label == 1 else 0.05\n",
    "\n",
    "    def _handle_search(self, action):\n",
    "        idx = action - 11\n",
    "        entities = self._topk_entities()\n",
    "        if idx >= len(entities):\n",
    "            return -0.1\n",
    "\n",
    "        entity = entities[idx]\n",
    "        self.seen_entities.add(entity)\n",
    "\n",
    "        return 0.1\n",
    "\n",
    "    def _handle_expand(self, action):\n",
    "        if action == 16:\n",
    "            self.cursor = max(0, self.cursor - 1)\n",
    "        elif action == 17:\n",
    "            self.cursor = min(len(self.logs) - self.window_size, self.cursor + 1)\n",
    "        elif action == 18:\n",
    "            self.cursor = max(0, self.cursor - 1)\n",
    "            self.cursor = min(len(self.logs) - self.window_size, self.cursor + 1)\n",
    "\n",
    "    def _final_reward(self):\n",
    "        # session-level reward（F1 / consistency / LLM judge）\n",
    "        return len(self.labeled_logs) * 0.1\n",
    "\n",
    "    def _get_state(self):\n",
    "        obs = self._extract_observation_features()\n",
    "        mem = self._extract_memory_features()\n",
    "        prog = np.array([\n",
    "            self.steps / self.max_steps,\n",
    "            len(self.labeled_logs),\n",
    "            len(self.seen_entities)\n",
    "        ])\n",
    "\n",
    "        state = np.concatenate([obs, mem, prog])\n",
    "        return state.astype(np.float32)\n",
    "\n",
    "    # ===== Feature Extractors =====\n",
    "\n",
    "    def _extract_observation_features(self):\n",
    "        # 回傳固定長度向量（例如 40 維）\n",
    "        return np.zeros(40)\n",
    "\n",
    "    def _extract_memory_features(self):\n",
    "        # 例如 21 維\n",
    "        return np.zeros(21)\n",
    "\n",
    "    def _topk_logs(self):\n",
    "        return self.logs[self.cursor:self.cursor + self.window_size]\n",
    "\n",
    "    def _topk_entities(self):\n",
    "        # 從目前 window 抽 entity\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7742c",
   "metadata": {},
   "source": [
    "#### 主要流程 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02672e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
